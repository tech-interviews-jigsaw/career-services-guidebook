{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a13357e2-2945-44f3-9ce0-ab98d4ba3f59",
   "metadata": {},
   "source": [
    "# Understanding your skills"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42df05d9-930f-4da1-a841-2f46aaebedc6",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18370550-6ecc-4b0f-9f0e-636f01bb8390",
   "metadata": {},
   "source": [
    "You know a lot.  You've learned backend engineer, cloud computing, how to build serverless data pipeliness, more classic data pipelines with airflow, and ELT pipelines with DBT.  You can analyze data and perform data storytelling.  And you've already added value to an organization through your externship.  \n",
    "\n",
    "You've earned the skills necessary to qualify as a data engineer but a range of positions in the data world.\n",
    "\n",
    "Let's begin by reviewing your skills as a data engineer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04ec558-888d-4076-ac38-84c0018f2f89",
   "metadata": {},
   "source": [
    "### Your data engineering skills"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1269d00e-4a5c-4265-a58d-6f70d72d96d2",
   "metadata": {},
   "source": [
    "Below, we'll use the pyramid principal to describe our data engineering skillset.\n",
    "\n",
    "We see the data engineering skillset as a being composed of backend engineering, cloud computing, and data pipelines.  \n",
    "\n",
    "Below you can see this.  \n",
    "\n",
    "<img src=\"./data-eng-pipeline.png\">\n",
    "\n",
    "1. Backend engineering\n",
    "\n",
    "To the left is the Python ETL work involved in pulling data from an API, by scraping HTML, or by pulling it from an OLTP database and storing either directly to an analytics database, or in this case, to a data lake in S3 (so someone like a data scientist can query it).\n",
    "\n",
    "2. Coud computing\n",
    "\n",
    "We deployed this code to the cloud using both Docker (which stores our code and related dependencies (eg. Python version, pip libraries, commands to get running), and AWS as cloud provider which *hosts* our docker image.\n",
    "\n",
    "3. Data pipelines\n",
    "\n",
    "By data pipelines we mean developing a system that extracts, stores, and transforms our data (ELT in this case).  And here we use s3 as our data lake for data scientists, then move the data to our data warehouse, and repeatedly transform it with DBT.  The end goal is to transform the data until we can deliver to internal stakeholders who may be less technical than our data scientists.  And to do so in the form of CSV files (called reports) or dashboards (eg. tableau, PowerBI)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe6d944-37f4-4472-a8e5-0194eb2d3efb",
   "metadata": {},
   "source": [
    "#### Serverless data pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eeaafc5-1b3b-4ac1-b8a6-f8fb2db2ae54",
   "metadata": {},
   "source": [
    "Remember that we also developed a [serverless data pipeline](https://github.com/data-engineering-jigsaw/airflow-fullstack-etl/tree/solution/codebase).\n",
    "\n",
    "<img src=\"./serverless-pipeline.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4e7930-988f-4abf-bd8d-63ba3bba13fb",
   "metadata": {},
   "source": [
    "The overall structure is really the same, but in this case, we first store our raw data to s3 (for say data scientists), then transform our data to be more structured and store that structured data s3, so that we can ultimately load it into an analytics database.  From there, we can transform it further with DBT.  To keep each of these steps independent, and have the ability to invoke them independently we wrap each step in a separate docker container that can be invoked through a lambda function.  We use airflow as our orchestrator to invoke these lambda functions in seequence.  And DBT to further transform the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4783588e-c9ab-4dbf-9241-9346064403e0",
   "metadata": {},
   "source": [
    "And again, we can end by displaying the data in a dashboard like tableau. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2706755f-b10d-4ac7-832e-ba6048d40b84",
   "metadata": {},
   "source": [
    "#### What about data analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb21f2ff-0e1a-494d-8606-d25806a33ee5",
   "metadata": {},
   "source": [
    "Where does the data analysis come in?  Really in three different places.\n",
    "\n",
    "<img src=\"./serverless-pipeline.png\" width=\"80%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c564ff4e-18a0-4936-bbe6-42bbd73a4607",
   "metadata": {},
   "source": [
    "1. Exploring the source layer/potential sources\n",
    "\n",
    "* Before we set up a pipeline to repeatedly pull data from a source like the Amadeus API, we may want to explore that dataset to see if it is worth pulling data from.  There we can use our skills of checking the completeness/representativeness of the data, and whether the features help to explain any target (like revenue, etc.)\n",
    "\n",
    "2. The data lake (raw data)\n",
    "\n",
    "Remember that the data lake is a place we can store our raw unprocessed data.  This is another good place to explore, because we can use data exploration to determine what features are worth transforming and ultimately providing to external stakeholders.\n",
    "\n",
    "3. Data Dashboards/CSV files\n",
    "\n",
    "Finally, we'll have less data to work with, but we can also explore our data at the end of our pipeline, when we are presenting data with our data dashboards or other visualizations.  Finally, this is prime space for our data storytelling skills.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6d119db-c3f5-4426-88cd-39fd46c9a14d",
   "metadata": {},
   "source": [
    "### Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078059f2-444b-4b5a-a38b-7adc823faab5",
   "metadata": {},
   "source": [
    "In this lesson, we described our data engineering skills by thinking through our end to end data pipeline.\n",
    "\n",
    "<img src=\"./data-eng-pipeline.png\" width=\"70%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6803f697-1bfd-4d4a-8045-5695b41dea2b",
   "metadata": {},
   "source": [
    "As we saw, we can summarize our data engineering skills as (1) backend engineering (Python, SQL, Flask, Object oriented design), (2) cloud computing (AWS, Docker, Bash), and (3) data pipelines (Airflow, DBT).\n",
    "\n",
    "We built a serverless data pipeline which involved loading raw source data into s3 (in our data lake), and then transforming that data into structured data, before loading it to an analytics database for further transformation through DBT until it is in OLAP form and ready to present to stakeholders through a dashboard, CSV file, or OLAP data model.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
